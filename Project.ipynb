{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "379iBfXBaLYZ"
      },
      "source": [
        "# **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOQYHVxNxEml",
        "outputId": "ef65b1e8-a8de-41c8-8466-9a0fd020ac51"
      },
      "source": [
        "pip install keras_tuner"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.0.3-py3-none-any.whl (96 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 96 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (1.19.5)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt-legacy-1.0.3.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (2.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (21.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (5.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (2.23.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (5.0.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (57.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (2.6.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras_tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras_tuner) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->keras_tuner) (0.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras_tuner) (2.4.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras_tuner) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (2021.5.30)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.32.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.34.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (3.3.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.4.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.12.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras_tuner) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras_tuner) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras_tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras_tuner) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras_tuner) (3.7.4.3)\n",
            "Building wheels for collected packages: kt-legacy\n",
            "  Building wheel for kt-legacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kt-legacy: filename=kt_legacy-1.0.3-py3-none-any.whl size=9568 sha256=02c9b425cd2ac04a18510e026a8da6ce0e767b9367a36d203c4eceea7ccdf73c\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/5c/e0/13003e68c17f403af40b92a24d20171b95fef13b0fdaba833c\n",
            "Successfully built kt-legacy\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.0.3 kt-legacy-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF66N3VCZ5tl"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "from __future__ import print_function\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from numpy import concatenate\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "\n",
        "from torch.optim import Adam, SGD\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.semi_supervised import LabelPropagation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix, roc_auc_score, average_precision_score\n",
        "import time\n",
        "import itertools\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras.datasets import cifar100\n",
        "\n",
        "import keras_tuner as kt\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg19 import VGG19\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_do-UKu8HCE",
        "outputId": "04df6f3b-3fad-4542-bfad-a6b3353173c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-lGVmYX7chk"
      },
      "source": [
        "# **Loading data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhoOjliz8D6H"
      },
      "source": [
        "! unzip /content/drive/MyDrive/CIFAR100.zip -d cifar100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6ieGrAG8ZME"
      },
      "source": [
        "def unpickle(file):\n",
        "    \n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "        \n",
        "    return dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynQG5a7C8m5k"
      },
      "source": [
        "**Create data table**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ydNXd4u8oVM"
      },
      "source": [
        "path_train = '/content/cifar100/train'\n",
        "train_dict = unpickle(path_train)\n",
        "\n",
        "path = '/content/cifar100/meta'\n",
        "names_dict = unpickle(path)\n",
        "\n",
        "fine_labels_list = train_dict[b'fine_labels']\n",
        "coarse_labels_list = train_dict[b'coarse_labels']\n",
        "data_list = train_dict[b'data']\n",
        "\n",
        "fine_label_names_list = names_dict[b'fine_label_names']\n",
        "coarse_label_names_list = names_dict[b'coarse_label_names']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEzO5lJy8qod"
      },
      "source": [
        "# Create the df_train dataframe\n",
        "df_train = pd.DataFrame(fine_labels_list, columns=['fine_labels'])\n",
        "\n",
        "# Create new columns\n",
        "df_train['coarse_labels'] = coarse_labels_list\n",
        "df_train['image_num'] = df_train.index\n",
        "\n",
        "# Create the image_id column\n",
        "def create_imageid(row):\n",
        "    image_id = str(row['image_num']) +'.jpg'\n",
        "    return image_id\n",
        "\n",
        "df_train['image_id'] = df_train.apply(create_imageid, axis=1)\n",
        "\n",
        "# Create the fine and coarse label names columns\n",
        "def create_finelabelname(x):\n",
        "    # this returns bytes: b'apple'\n",
        "    name = fine_label_names_list[x]\n",
        "    # convert bytes to string: 'apple'\n",
        "    name = name.decode(\"utf-8\") \n",
        "    return name\n",
        "\n",
        "def create_coarselabelname(x):\n",
        "    # this returns bytes: b'apple'\n",
        "    name = coarse_label_names_list[x]\n",
        "    # convert bytes to string: 'apple'\n",
        "    name = name.decode(\"utf-8\") \n",
        "    return name\n",
        "\n",
        "df_train['fine_label_names'] = df_train['fine_labels'].apply(create_finelabelname)\n",
        "df_train['coarse_label_names'] = df_train['coarse_labels'].apply(create_coarselabelname)\n",
        "df_train['data'] = data_list.tolist()\n",
        "\n",
        "# Remove unnecessary columns\n",
        "df_train = df_train.drop('image_num', axis=1)\n",
        "\n",
        "# Reorder the columns\n",
        "cols = ['image_id', 'fine_label_names', 'fine_labels', 'coarse_label_names', 'coarse_labels', 'data']\n",
        "df_train = df_train[cols]\n",
        "\n",
        "df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcPxH2TAFRlw"
      },
      "source": [
        "def change_to_array(X_train, X_test):\n",
        "    new_data_train = []\n",
        "    new_data_test = []\n",
        "    X_data1 = np.array(X_train)\n",
        "    X_data3 = np.array(X_test)\n",
        "    for val in X_data1:\n",
        "        new_data_train.append(val)\n",
        "    for val in X_data3:\n",
        "        new_data_test.append(val)\n",
        "    return  np.array(new_data_train),np.array(new_data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcbu6V_t7Nhu"
      },
      "source": [
        "def split_data(df,num_classes):\n",
        "  X=df['data']\n",
        "  y=df['fine_label_names']\n",
        "\n",
        "  x_train, x_test, y_train, y_test=train_test_split(X,y,test_size=0.2)\n",
        "  x_train, x_test=change_to_array(x_train, x_test)\n",
        "\n",
        "  x_train = x_train.reshape(len(x_train),3,32,32).transpose(0,2,3,1)\n",
        "  x_test = x_test.reshape(len(x_test),3,32,32).transpose(0,2,3,1)\n",
        "\n",
        "  # Normalize the data. Before we need to connvert data type to float for computation.\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "\n",
        "  # Convert class vectors to binary class matrices. This is called one hot encoding.\n",
        "  y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "  y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "  return x_train, x_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lkB65SIa16s"
      },
      "source": [
        "#MODEL I\n",
        "**Defining the model architecture**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuxLdca29jGB"
      },
      "source": [
        "**parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woQIdnkq9iPX"
      },
      "source": [
        "n_folds=10\n",
        "epochs=7\n",
        "batch_size=32  #The default batch size of keras.\n",
        "num_classes=10\n",
        "#x_train.shape[1:]\n",
        "input_shape=(32, 32, 3)\n",
        "model_II=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUZc-B1Ha3wm"
      },
      "source": [
        "def build_model(hp):\n",
        "\n",
        "  #define the convnet\n",
        "  model = Sequential()   \n",
        "\n",
        "  model.add(Conv2D(\n",
        "                  filters=16,\n",
        "                  kernel_size=3,\n",
        "                  padding='same',\n",
        "                  activation='relu',\n",
        "                  input_shape=input_shape))\n",
        "  model.add(Conv2D(\n",
        "                  filters=16,\n",
        "                  padding='same',\n",
        "                  activation='relu',\n",
        "                  kernel_size=3))\n",
        "  model.add(MaxPooling2D(pool_size=2))\n",
        "  model.add(Dropout(rate=0.025))\n",
        "  model.add(Conv2D(\n",
        "                  filters=32,\n",
        "                  kernel_size=3,\n",
        "                  padding='same',\n",
        "                  activation='relu'))\n",
        "  model.add(Conv2D(\n",
        "                  filters=64,\n",
        "                  kernel_size=3,\n",
        "                  padding='same',\n",
        "                  activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=2,padding='same'))\n",
        "  model.add(Dropout(rate=0.025))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units=128, activation='relu'))\n",
        "  model.add(Dropout(rate=0.025))\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  if model_II==False:\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.SGD(\n",
        "            hp.Choice('learning_rate',\n",
        "                      values=[1e-2, 1e-3, 1e-4])),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "  else:\n",
        "      model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate',\n",
        "                      values=[1e-2, 1e-3, 1e-4])),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYJjGs9hbu4M"
      },
      "source": [
        " **Model training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkjJE0FIbwrt"
      },
      "source": [
        "def hyperparameters(x_train, y_train):\n",
        "  tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    seed=1,\n",
        "    max_trials=50,\n",
        "    executions_per_trial=2,\n",
        "    directory='search',\n",
        "    project_name='cifar100') \n",
        "\n",
        "  tuner.search(x_train, y_train,\n",
        "             epochs=3,\n",
        "             validation_split=0.1) \n",
        "\n",
        "  # Get the optimal hyperparameters\n",
        "  best_model=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "  model = tuner.hypermodel.build(best_model)\n",
        "\n",
        "  return model,best_model.get('learning_rate')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FAV0DVC7RhS"
      },
      "source": [
        "def calculateMeasures(model, x_test, y_test):\n",
        "  measures = []\n",
        "\n",
        "  start_time = time.time()\n",
        "  y_pred=model.predict(x_test)\n",
        "  measures.append(time.time()-start_time)\n",
        "\n",
        "  pred=np.argmax(y_pred,axis=1)\n",
        "  ground = np.argmax(y_test,axis=1)\n",
        "  cnf_matrix = confusion_matrix(ground, pred)\n",
        "  \n",
        "  FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
        "  FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "  TP = np.diag(cnf_matrix)\n",
        "  TN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "\n",
        "  FP = FP.astype(float)\n",
        "  FN = FN.astype(float)\n",
        "  TP = TP.astype(float)\n",
        "  TN = TN.astype(float)\n",
        "\n",
        "  #accuracy\n",
        "  measures.append(accuracy_score(ground, pred))\n",
        "  print(\"accuracy: \", measures[1])\n",
        "\n",
        "  # Sensitivity, hit rate, recall, or true positive rate\n",
        "  measures.append(recall_score(ground, pred, average=\"macro\"))\n",
        "  print(\"TPR-recall: \",measures[2])\n",
        "\n",
        "  # Fall out or false positive rate\n",
        "  FPR = FP/(FP+TN)\n",
        "  FPR = np.average(FPR)\n",
        "  measures.append(FPR)\n",
        "  print(\"FPR: \",measures[3])\n",
        "\n",
        "  #precision\n",
        "  measures.append(precision_score(ground, pred, average=\"macro\"))\n",
        "  print(\"precision: \", measures[4])\n",
        "\n",
        "  #AUC – Area Under the ROC Curve\n",
        "  measures.append(roc_auc_score(y_test, model.predict_proba(x_test), multi_class='ovr'))\n",
        "  print(\"AUC: \", measures[5])\n",
        "\n",
        "  measures.append(average_precision_score(y_test, y_pred, average=\"micro\"))\n",
        "  print(\"PR-Curve: \", measures[6])\n",
        "  return measures\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Skfj3XvzxvU"
      },
      "source": [
        "def run(model,x_train, y_train,x_test,y_test):\n",
        "    fit_model_log=model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "    test_score = model.evaluate(np.array([x for x in x_test]), y_test)\n",
        "\n",
        "    print(model.metrics_names[0] + \":\"+ str(test_score[0]))\n",
        "    print(model.metrics_names[1] + \":\"+ str(test_score[1]))\n",
        "    measures = calculateMeasures(model, x_test, y_test)\n",
        "    return fit_model_log, measures"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVkLrRuW-ZUm"
      },
      "source": [
        "**Train the model with K-fold Cross Val**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-0mB0Eq7LXx"
      },
      "source": [
        "def run_model(datasetName, df, num_classes, results):\n",
        "  model_history = [] \n",
        "  for i in range(n_folds):\n",
        "    print(\"Training on Fold: \",i+1)\n",
        "    # split into train and test\n",
        "    x_train, x_test, y_train, y_test=split_data(df,num_classes)\n",
        "\n",
        "    if model_II:\n",
        "      x_train, x_test = normalize(x_train, x_test)\n",
        "\n",
        "    # split train into labeled and unlabeled\n",
        "    x_train_lab, x_test_unlab, y_train_lab, y_test_unlab = train_test_split(x_train, y_train, test_size=0.50, random_state=1, stratify=y_train)\n",
        "    # create the training dataset input\n",
        "    nsamples, nx, ny,nz = x_train_lab.shape\n",
        "    x_train_lab = x_train_lab.reshape((nsamples,nx*ny*nz))\n",
        "\n",
        "    nsamples, nx, ny,nz = x_test_unlab.shape\n",
        "    x_test_unlab = x_test_unlab.reshape((nsamples,nx*ny*nz))\n",
        "\n",
        "    x_train_mixed = concatenate((x_train_lab, x_test_unlab))\n",
        "\n",
        "    # create \"no label\" for unlabeled data\n",
        "    nolabel=[-1 for _ in range(len(y_test_unlab))]\n",
        "    y_train_lab=np.argmax(y_train_lab, axis=-1)\n",
        "    # recombine training dataset labels\n",
        "    y_train_mixed = concatenate((y_train_lab, nolabel))\n",
        "\n",
        "    start_time = time.time()\n",
        "    # define model\n",
        "    model = LabelPropagation()\n",
        "    # fit model on training dataset\n",
        "    model.fit(x_train_mixed, y_train_mixed)\n",
        "    # get labels for entire training dataset data\n",
        "    tran_labels = model.transduction_\n",
        "\n",
        "    tran_labels=keras.utils.to_categorical(tran_labels, num_classes)\n",
        "    x_train_mixed = x_train_mixed.reshape(len(x_train_mixed),3,32,32).transpose(0,2,3,1)\n",
        "\n",
        "\n",
        "    # define supervised learning model\n",
        "    best_model_hp,lr=hyperparameters(x_train_mixed, tran_labels)\n",
        "    print(lr)\n",
        "    fit_model_log, measures = run(best_model_hp,x_train_mixed, tran_labels,x_test,y_test)\n",
        "    end_time = time.time() - start_time\n",
        "    model_history.append(fit_model_log)\n",
        "    print(\"=======\"*12, end=\"\\n\\n\\n\")\n",
        "    results.loc[len(results)] = np.array([datasetName, 'label propagate', i+1, lr, measures[1], measures[2], measures[3], measures[4], measures[5], measures[6], end_time , measures[0]])\n",
        "\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_RaFdy-2TYj"
      },
      "source": [
        "**run model on 20 dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEG0yV8XB0Sx"
      },
      "source": [
        "df_train['coarse_label_names'].value_counts()\n",
        "labels=df_train['coarse_label_names'].unique()\n",
        "columnsName=['Dataset Name', 'Algoritem Name', 'Cross Validation', 'Hyper-Parameters Values', 'Accuracy', 'TPR', 'FPR', 'Precision', 'AUC', 'PR-Curve', 'Training Time', 'Inference Time']\n",
        "results = pd.DataFrame(columns=columnsName)\n",
        "datasetsName = []\n",
        "# create excel writer object\n",
        "writer = pd.ExcelWriter('output_model1.xlsx')\n",
        "\n",
        "\n",
        "#df_x\n",
        "for x in range(len(labels)):\n",
        "  if x < 19:\n",
        "    globals()[\"df_\"+str(x)] = df_train.drop(df_train[(df_train.coarse_label_names != labels[x]) & (df_train.coarse_label_names != labels[x+1])].index)\n",
        "    datasetName = 'Cifar100-'+labels[x]+'-'+labels[x+1]\n",
        "  else:\n",
        "    globals()[\"df_\"+str(x)] = df_train.drop(df_train[(df_train.coarse_label_names != labels[x]) & (df_train.coarse_label_names != labels[0])].index)\n",
        "    datasetName = 'Cifar100-'+labels[x]+'-'+labels[0]\n",
        "  datasetsName.append(datasetName)\n",
        "  globals()[\"df_\"+str(x)].fine_label_names =pd.factorize(globals()[\"df_\"+str(x)].fine_label_names)[0]\n",
        "  results = run_model(datasetName, globals()[\"df_\"+str(x)],num_classes, results)\n",
        "  # write dataframe to excel\n",
        "  results.to_excel(writer)\n",
        "  # save the excel\n",
        "  writer.save()\n",
        "\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNvT1TrporMl"
      },
      "source": [
        "# **Model II**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agEMHozqqusS"
      },
      "source": [
        "**Improvement to the algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n3zYrf10YEb"
      },
      "source": [
        "def normalize(X_train,X_test):\n",
        "  #this function normalize inputs for zero mean and unit variance\n",
        "  # it is used when training a model.\n",
        "  # Input: training set and test set\n",
        "  # Output: normalized training set and test set according to the trianing set statistics.\n",
        "  mean = np.mean(X_train,axis=(0,1,2,3))\n",
        "  std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "  # print(mean)\n",
        "  # print(std)\n",
        "  X_train = (X_train-mean)/(std+1e-7)\n",
        "  X_test = (X_test-mean)/(std+1e-7)\n",
        "  return X_train, X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oomvo1_douWE"
      },
      "source": [
        "model_II=True\n",
        "df_train['coarse_label_names'].value_counts()\n",
        "labels=df_train['coarse_label_names'].unique()\n",
        "columnsName=['Dataset Name', 'Algoritem Name', 'Cross Validation', 'Hyper-Parameters Values', 'Accuracy', 'TPR', 'FPR', 'Precision', 'AUC', 'PR-Curve', 'Training Time', 'Inference Time']\n",
        "results = pd.DataFrame(columns=columnsName)\n",
        "datasetsName = []\n",
        "# create excel writer object\n",
        "writer = pd.ExcelWriter('output_model2.xlsx')\n",
        "\n",
        "for x in range(len(labels)):\n",
        "  print(x)\n",
        "  if x < 19:\n",
        "    globals()[\"df_\"+str(x)] = df_train.drop(df_train[(df_train.coarse_label_names != labels[x]) & (df_train.coarse_label_names != labels[x+1])].index)\n",
        "    datasetName = 'Cifar100-'+labels[x]+'-'+labels[x+1]\n",
        "  else:\n",
        "    globals()[\"df_\"+str(x)] = df_train.drop(df_train[(df_train.coarse_label_names != labels[x]) & (df_train.coarse_label_names != labels[0])].index)\n",
        "    datasetName = 'Cifar100-'+labels[x]+'-'+labels[0]\n",
        "  datasetsName.append(datasetName)\n",
        "  globals()[\"df_\"+str(x)].fine_label_names =pd.factorize(globals()[\"df_\"+str(x)].fine_label_names)[0]\n",
        "  results = run_model(datasetName, globals()[\"df_\"+str(x)],num_classes, results)\n",
        "  # write dataframe to excel\n",
        "  results.to_excel(writer)\n",
        "  # save the excel\n",
        "  writer.save()\n",
        "results\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQY376-7cpyq"
      },
      "source": [
        "#**Model III**\n",
        "\n",
        "**base model used is VGG19**- The pretrained weights from the imagenet challenge are used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJZVubhccpPO"
      },
      "source": [
        "base_model = VGG19(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=5)\n",
        "#Lets add the final layers to these base models where the actual classification is done in the dense layers\n",
        "\n",
        "def build_model_3(hp):\n",
        "  model_3= Sequential()\n",
        "  model_3.add(base_model) #Adds the base model (in this case vgg19 to model_3)\n",
        "  model_3.add(Flatten()) #Since the output before the flatten layer is a matrix we have to use this function to get a vector of the form nX1 to feed it into the fully connected layers\n",
        "  model_3.add(Dense(5,activation=('softmax'))) #This is the classification layer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  model_3.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate',\n",
        "                      values=[1e-2, 1e-3, 1e-4])),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "  return model_3\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX8XE2_4gLpR"
      },
      "source": [
        "def hyperparameters_3(x_train, y_train):\n",
        "  tuner = kt.RandomSearch(\n",
        "    build_model_3,\n",
        "    objective='val_accuracy',\n",
        "    seed=1,\n",
        "    max_trials=50,\n",
        "    executions_per_trial=2,\n",
        "    directory='random_search',\n",
        "    project_name='cifar100') \n",
        "\n",
        "  tuner.search(x_train, y_train,\n",
        "             epochs=3,\n",
        "             validation_split=0.1) \n",
        "\n",
        "  # Get the optimal hyperparameters\n",
        "  best_model=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "  model = tuner.hypermodel.build(best_model)\n",
        "\n",
        "  return model,best_model.get('learning_rate')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY1L01hdfl1-"
      },
      "source": [
        "def run_3(model,x_train, y_train,x_test,y_test):\n",
        "    fit_model_log=model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "    test_score = model.evaluate(np.array([x for x in x_test]), y_test)\n",
        "    print(model.metrics_names[0] + \":\"+ str(test_score[0]))\n",
        "    print(model.metrics_names[1] + \":\"+ str(test_score[1]))\n",
        "    measures = calculateMeasures(model, x_test, y_test)\n",
        "    return fit_model_log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4xYKHXNf6vR"
      },
      "source": [
        "def run_model_3(datasetName,df,num_classes,results):\n",
        "  model_history = [] \n",
        "  for i in range(n_folds):\n",
        "    print(\"Training on Fold: \",i+1)\n",
        "    # split into train and test\n",
        "    x_train, x_test, y_train, y_test=split_data(df,num_classes)\n",
        "    # split train into labeled and unlabeled\n",
        "    x_train_lab, x_test_unlab, y_train_lab, y_test_unlab = train_test_split(x_train, y_train, test_size=0.50, random_state=1, stratify=y_train)\n",
        "    # create the training dataset input\n",
        "    nsamples, nx, ny,nz = x_train_lab.shape\n",
        "    x_train_lab = x_train_lab.reshape((nsamples,nx*ny*nz))\n",
        "\n",
        "    nsamples, nx, ny,nz = x_test_unlab.shape\n",
        "    x_test_unlab = x_test_unlab.reshape((nsamples,nx*ny*nz))\n",
        "\n",
        "    x_train_mixed = concatenate((x_train_lab, x_test_unlab))\n",
        "\n",
        "    # create \"no label\" for unlabeled data\n",
        "    nolabel=[-1 for _ in range(len(y_test_unlab))]\n",
        "    y_train_lab=np.argmax(y_train_lab, axis=-1)\n",
        "    # recombine training dataset labels\n",
        "    y_train_mixed = concatenate((y_train_lab, nolabel))\n",
        "    start_time=time.time()\n",
        "    # define model\n",
        "    model = LabelPropagation()\n",
        "    # fit model on training dataset\n",
        "    model.fit(x_train_mixed, y_train_mixed)\n",
        "    # get labels for entire training dataset data\n",
        "    tran_labels = model.transduction_\n",
        "\n",
        "    tran_labels=keras.utils.to_categorical(tran_labels, num_classes)\n",
        "    x_train_mixed = x_train_mixed.reshape(len(x_train_mixed),3,32,32).transpose(0,2,3,1)\n",
        "\n",
        "\n",
        "    # define supervised learning model\n",
        "    best_model_hp,lr=hyperparameters_3(x_train_mixed, tran_labels)\n",
        "    print(lr)\n",
        "    fit_model_log, measures = run_3(best_model_hp,x_train_mixed, tran_labels,x_test,y_test)\n",
        "    end_time=time.time()-start_time\n",
        "    model_history.append(fit_model_log)\n",
        "    print(\"=======\"*12, end=\"\\n\\n\\n\")\n",
        "    results.loc[len(results)] = np.array([datasetName, 'CNN- ResNet50', i+1, lr, measures[1], measures[2], measures[3], measures[4], measures[5], measures[6], end_time , measures[0]])\n",
        "\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4_nf6OJugUM7"
      },
      "source": [
        "df_train['coarse_label_names'].value_counts()\n",
        "labels=df_train['coarse_label_names'].unique()\n",
        "columnsName=['Dataset Name', 'Algoritem Name', 'Cross Validation', 'Hyper-Parameters Values', 'Accuracy', 'TPR', 'FPR', 'Precision', 'AUC', 'PR-Curve', 'Training Time', 'Inference Time']\n",
        "results = pd.DataFrame(columns=columnsName)\n",
        "datasetsName = []\n",
        "\n",
        "#df_x\n",
        "for x in range(len(labels)):\n",
        "  print(x)\n",
        "  if x < 19:\n",
        "    globals()[\"df_\"+str(x)] = df_train.drop(df_train[(df_train.coarse_label_names != labels[x]) & (df_train.coarse_label_names != labels[x+1])].index)\n",
        "    datasetName = 'Cifar100-'+labels[x]+'-'+labels[x+1]\n",
        "  else:\n",
        "    globals()[\"df_\"+str(x)] = df_train.drop(df_train[(df_train.coarse_label_names != labels[x]) & (df_train.coarse_label_names != labels[0])].index)\n",
        "    datasetName = 'Cifar100-'+labels[x]+'-'+labels[0]\n",
        "  datasetsName.append(datasetName)\n",
        "  globals()[\"df_\"+str(x)].fine_label_names =pd.factorize(globals()[\"df_\"+str(x)].fine_label_names)[0]\n",
        "  results = run_model_3(datasetName, globals()[\"df_\"+str(x)],num_classes, results)\n",
        "\n",
        "results\n",
        "  \n",
        "# create excel writer object\n",
        "writer = pd.ExcelWriter('output_model3.xlsx')\n",
        "# write dataframe to excel\n",
        "results.to_excel(writer)\n",
        "# save the excel\n",
        "writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mgd1-h-Yx47"
      },
      "source": [
        "# **Statistical significance testing of the results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxPLq9zzY5Ps"
      },
      "source": [
        "**Friedman test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyc-yIXHY-1T"
      },
      "source": [
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "# Loading the example DataFrame.\n",
        "performances = pd.read_csv('/content/drive/MyDrive/AUCResults.csv')\n",
        "# First, we extract the algorithms names.\n",
        "algorithms_names = performances.drop('Dataset Name', axis=1).columns\n",
        "# Then, we extract the performances as a numpy.ndarray.\n",
        "performances_array = performances[algorithms_names].values\n",
        "# Finally, we apply the Friedman test.\n",
        "friedmanchisquare(*performances_array)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}